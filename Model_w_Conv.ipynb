{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "class SSVM_Conv:\n",
    "    def __init__(self, A_pos, A_neg, C, w0, b0, delta, beta, con_plot=True, tol=1e-5, convergSpeed=1e-8, maxIter=100, alpha=1000):\n",
    "\n",
    "        self.tol = tol\n",
    "        self.convergSpeed = convergSpeed\n",
    "        self.C = C\n",
    "        self.maxIter = maxIter\n",
    "        self.alpha = alpha # p parameter\n",
    "\n",
    "        # loss function parameters\n",
    "        self.delta = delta # -0.8\n",
    "        self.beta = beta # 0.4\n",
    "\n",
    "        # convergence plot\n",
    "        self.con_plot = con_plot\n",
    "\n",
    "        try:\n",
    "            self.A = np.vstack([np.hstack([A_pos, -np.ones([A_pos.shape[0], 1])]),\n",
    "                                np.hstack([-A_neg, np.ones([A_neg.shape[0], 1])]),])\n",
    "            self.w = np.vstack((w0, b0))\n",
    "        except:\n",
    "            print(\"\\n===Error in SSVM-init : the dimension of w, b, A_pos, A_neg not agree===\")\n",
    "            sys.exit(1)\n",
    "\n",
    "\n",
    "    def objf(self, w):\n",
    "        '''Evaluate the function value:\n",
    "            w = vector in SVM\n",
    "            Output:\n",
    "            return = value\n",
    "        '''\n",
    "\n",
    "        try:\n",
    "            margin_adj_loss = self.beta * np.exp(-self.delta * np.abs(self.A.dot(w))) \n",
    "            x =  np.ones((self.A.shape[0], 1)) - self.A.dot(w) + margin_adj_loss\n",
    "        except:\n",
    "            print(\"\\n===Error in SSVM-objf : loss function error===\")\n",
    "            sys.exit(1)\n",
    "        try:\n",
    "            # temp = (np.ones((self.A.shape[0], 1)) - self.A.dot(w))\n",
    "            # v = np.maximum(temp, 0)\n",
    "            # x = (np.ones((self.A.shape[0], 1)) - self.A.dot(w))\n",
    "            # print(np.log(1 + np.exp(-self.alpha * x)))\n",
    "            # print(x + 1/self.alpha )\n",
    "            v = x + 1/self.alpha * np.log(1 + np.exp(-self.alpha * x))\n",
    "\n",
    "            # hinge loss + regularization\n",
    "            return 0.5 * (v.T.dot(v) + w.T.dot(w) / self.C)\n",
    "        except TypeError:\n",
    "            print(\"\\n===Error in SSVM-objf : type of parameter are not the same===\")\n",
    "            sys.exit(1)\n",
    "\n",
    "\n",
    "    def zoomInt(self, phi, dphi, alpha_l, alpha_h, c1, c2, trail_step = 'bisection'):\n",
    "        ''' The purpose of this algorithm is to find a suitable step length (alpha) \n",
    "        that satisfies the Wolfe conditions during a line search in an optimization problem.\n",
    "        Successively decrease the inverval size of the alphas until acceptable step length found'''\n",
    "\n",
    "        tol = np.finfo(float).eps         \n",
    "        # Structure containing information about the iteration\n",
    "        info = {'alpha_ls': [], 'alpha_hs': [], 'alpha_js': [], 'phi_js': [], 'dphi_js': []}\n",
    "\n",
    "        n = 1\n",
    "        stop = False\n",
    "        max_iter = 100\n",
    "        alpha = 0\n",
    "\n",
    "        while n < max_iter and not stop:\n",
    "            # Find trial step length alpha_j in [alpha_l, alpha_h]\n",
    "            if trail_step.lower() == 'bisection':\n",
    "                alpha_j = 0.5 * (alpha_h + alpha_l)\n",
    "            elif trail_step.lower() == 'interp2':\n",
    "                # You need to implement the 'interp2' method\n",
    "                raise NotImplementedError(\"Interp2 method not implemented\")\n",
    "\n",
    "            phi_j = phi(alpha_j)\n",
    "\n",
    "            # Update info\n",
    "            info['alpha_ls'].append(alpha_l)\n",
    "            info['alpha_hs'].append(alpha_h)\n",
    "            info['alpha_js'].append(alpha_j)\n",
    "            info['phi_js'].append(phi_j)\n",
    "\n",
    "            if abs(alpha_h - alpha_l) < tol:\n",
    "                alpha = alpha_j\n",
    "                stop = True\n",
    "                print(\"Line search stopped because the interval became too small. Returning center of the interval.\")\n",
    "                print(f'Centre: {alpha_h - alpha_l}')\n",
    "                break\n",
    "\n",
    "            if phi_j > phi(0) + c1 * alpha_j * dphi(0) or phi(alpha_j) >= phi(alpha_l):\n",
    "                # alpha_j does not satisfy sufficient decrease condition look for alpha < alpha_j or phi(alpha_j) >= phi(alpha_l)\n",
    "                alpha_h = alpha_j\n",
    "                info['dphi_js'].append(np.nan)\n",
    "\n",
    "            else:\n",
    "                # alpha_j satisfies sufficient decrease condition\n",
    "                dphi_j = dphi(alpha_j)\n",
    "                info['dphi_js'].append(dphi_j)\n",
    "\n",
    "                if abs(dphi_j) <= -c2 * dphi(0):\n",
    "                    # alpha_j satisfies strong curvature condition\n",
    "                    alpha = alpha_j\n",
    "                    stop = True\n",
    "\n",
    "                elif dphi_j * (alpha_h - alpha_l) >= 0:\n",
    "                    # alpha_h : dphi(alpha_l)*(alpha_h - alpha_l) < 0\n",
    "                    # alpha_j violates this condition but swapping alpha_l <-> alpha_h will reestablish it\n",
    "                    alpha_h = alpha_l\n",
    "                \n",
    "                alpha_l = alpha_j\n",
    "\n",
    "            n += 1\n",
    "\n",
    "        return alpha, info\n",
    "            \n",
    "    def backtracking(self, deriv, x_k, p, alpha0, opts={'c1': 1e-4, 'rho': None}):\n",
    "\n",
    "        if opts['c1'] == None:\n",
    "            c1 = 1e-4\n",
    "        else:\n",
    "            c1 = opts['c1']\n",
    "        # rho = 0.1 for steepest descent, conjugate gradients\n",
    "        # rho = 0.9 for Newton, Quasi-Newton\n",
    "        if opts['rho'] == None:\n",
    "            rho = 0.2\n",
    "        else:\n",
    "            rho = opts['rho']\n",
    "        if alpha0 == None:\n",
    "            alpha0 = 1\n",
    "\n",
    "\n",
    "        # Initialize info structure\n",
    "        info = {'alphas': [alpha0], 'rho': [rho], 'c1': [c1]}\n",
    "\n",
    "        # Initial step length\n",
    "        alpha = alpha0\n",
    "\n",
    "        # Compute f, grad f at x_k\n",
    "        f_k = self.objf(x_k)\n",
    "        df_k = deriv(self, w_k=x_k)\n",
    "\n",
    "        # Backtracking line search\n",
    "        count = 0\n",
    "        print(self.objf(x_k + alpha * p), f_k + c1 * alpha * (df_k.T @ p))\n",
    "        while self.objf(x_k + alpha * p) >= f_k + c1 * alpha * (df_k.T @ p):\n",
    "            alpha = rho * alpha\n",
    "            info['alphas'].append(alpha)\n",
    "            count += 1\n",
    "            if count > 200:\n",
    "                break\n",
    "        \n",
    "        return alpha, info\n",
    "\n",
    "\n",
    "    def armijo(self, w, p_k, gap, obj1):\n",
    "        '''\n",
    "        Avoid the local maximum(minimum) in Newton method:\n",
    "        \n",
    "        w = current point\n",
    "        p_k = newton direction\n",
    "        gap = defined in ssvm code\n",
    "        obj1 = the object function value of current point\n",
    "        \n",
    "        Output:\n",
    "        stepsize = stepsize for Newton method\n",
    "        '''\n",
    "        diff = 0\n",
    "        stepsize = 0.5  # initial size\n",
    "        count = 1\n",
    "        try:\n",
    "            # Armijo: new function <= old function value - fraction * step length * directional derivative\n",
    "            # Wolfe sufficient decrease condition similar/same to this\n",
    "\n",
    "            # gap = p_k.T.dot(gradz). stopping criterion - If gap approaches zero, \n",
    "            # Newton direction aligns with the steepest descent direction, so small or negative gap means current solution \n",
    "            # close to the optimum, or the step might be too large if gap is negative.\n",
    "            # 0.05 = c1 in functions\n",
    "            while diff < -0.05 * stepsize * gap:\n",
    "                stepsize = 0.5 * stepsize # lambda = max{1,0.5,0.25...} as in paper\n",
    "                w2 = w + stepsize * p_k # (w^i+1, gamma^i+1) = (w_i, gamma_i) + step * direction_i\n",
    "                obj2 = self.objf(w2)\n",
    "                diff = obj1 - obj2 # f(w_i, gamma_i) - f(w^i+1, gamma^i+1) >= -delta * step * grad_f(w_i, gamma_i) * direction_i\n",
    "                \n",
    "                count = count + 1\n",
    "                if count > 20:\n",
    "                    break\n",
    "\n",
    "        except TypeError:\n",
    "            print(\"\\n===Error in SSVM-armijo : type of variables are not the same===\")\n",
    "            sys.exit(1)\n",
    "        except ValueError:\n",
    "            print(\"\\n===Error in SSVM-armijo : value of variables are not correct===\")\n",
    "\n",
    "        return stepsize\n",
    "    \n",
    "    def deriv(self, w_k=np.array([])):\n",
    "        return w_k / self.C - self.A[self.Point, :].T.dot(self.d[self.Point]) \n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        e = np.ones((self.A.shape[0], 1))\n",
    "        stopCond = 1\n",
    "        iter = 0\n",
    "        info = {'ws': [self.w], 'obj_fun': [self.objf(self.w)[0]], 'gradient': []}\n",
    "        while stopCond == False and iter < self.maxIter:\n",
    "            iter = iter + 1\n",
    "            margin_adj_loss = self.beta * np.exp(-self.delta * np.abs(self.A.dot(self.w))) \n",
    "            d = e - np.dot(self.A, self.w) + margin_adj_loss\n",
    "            Point = d[:, 0] > 0\n",
    "            self.Point = Point\n",
    "            self.d = d\n",
    "\n",
    "            if Point.all == False:\n",
    "                return\n",
    "        \n",
    "            # Regularisation gradient - loss function gradient\n",
    "            gradient = self.w / self.C - self.A[Point, :].T.dot(d[Point]) \n",
    "            hessian = np.eye(self.A.shape[1]) / self.C + self.A[Point, :].T.dot(self.A[Point, :]) \n",
    "\n",
    "            del d\n",
    "            del Point\n",
    "\n",
    "            if (gradient.T.dot(gradient) / self.A.shape[1]) > self.tol:\n",
    "                try:\n",
    "                    d2f_x_k_inv = np.linalg.inv(hessian) \n",
    "                    p_k = -np.dot(d2f_x_k_inv, gradient)\n",
    "                except:\n",
    "                    print(\"\\n===Error in SSVM-train : inverse of hessian error===\")\n",
    "                    p_k = np.zeros(self.w.shape)\n",
    "                \n",
    "                del hessian\n",
    "\n",
    "                obj1 = self.objf(self.w)\n",
    "                w1 = self.w + p_k\n",
    "                obj2 = self.objf(w1)\n",
    "                \n",
    "                if (obj1 - obj2) <= self.convergSpeed: \n",
    "                    # Strong Wolfe Step\n",
    "                    # Find the step size & Update to the new point\n",
    "                    stepsize, step_info = SSVM_Conv.backtracking(self, deriv=SSVM_Conv.deriv, x_k=w1, p=p_k, alpha0=0, opts={'c1': 1e-4, 'rho': 0.9}) \n",
    "                    # stepsize = self.armijo(self.w, p_k, gap, obj1)\n",
    "                    # print(step_info)\n",
    "                    # del step_info\n",
    "                    self.w = self.w + stepsize * p_k\n",
    "                else:\n",
    "                    # Use the Newton method\n",
    "                    self.w = w1\n",
    "\n",
    "                try:\n",
    "                    # stopCond = np.linalg.norm(p_k) #2-norm\n",
    "                    stopCond = (np.linalg.norm(gradient) < self.tol * (1 + abs(obj1)))\n",
    "                    print(stopCond)\n",
    "                except:\n",
    "                    print(\"\\n===Error in SSVM-train : 2norm of z error===\")\n",
    "                    sys.exit(1)\n",
    "\n",
    "                info['ws'].append(self.w)\n",
    "                info['obj_fun'].append(obj2[0])\n",
    "                info['gradient'].append(gradient)\n",
    "                \n",
    "                if stopCond == True or iter == self.maxIter:\n",
    "                    margin_adj_loss = self.beta * np.exp(-self.delta * np.abs(self.A.dot(self.w))) \n",
    "                    d = e - np.dot(self.A, self.w) + margin_adj_loss\n",
    "                    Point = d[:, 0] > 0\n",
    "                    info['final_obj_fun'] = obj2[0][0]\n",
    "                    info['final_hessian'] = np.eye(self.A.shape[1]) / self.C + self.A[Point, :].T.dot(self.A[Point, :]) \n",
    "                else:\n",
    "                    print('etnerede')\n",
    "                    info['final_obj_fun'] = 0\n",
    "                    info['final_hessian'] = 0\n",
    "\n",
    "\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if self.con_plot:\n",
    "            arr = np.zeros(shape=(len(info['ws']), self.w.shape[0]))\n",
    "            for i in range(len(info['ws'])):\n",
    "                arr[i,:] = info['ws'][i].T\n",
    "            \n",
    "            xMin = self.w\n",
    "            rows = arr.shape[0]\n",
    "\n",
    "            p = 2\n",
    "            M = info['final_hessian']  # M is the Hessian at the solution, M has to be symmetric + s.p.d - (17,17)\n",
    "            \n",
    "            # Convergence of iterates: || x_k - xMin ||_M\n",
    "            err = arr - np.tile(xMin.T, (rows, 1))\n",
    "            con_coeffs = [np.sqrt(np.dot(err[k].T, M.dot(err[k]))) for k in range(rows)] #(289, 289)\n",
    "            \n",
    "            # Convergence of function values: f(x_k) - f(xMin)\n",
    "            con_f = [info['obj_fun'][k][0] - info['final_obj_fun'] for k in range(rows)] #(1, 1)\n",
    "\n",
    "            # # Convergence of gradient: || f(x_k)||_p\n",
    "            con_df = [np.sum(np.abs(info['gradient'][k])**p)**(1/p) for k in range(rows-1)]\n",
    "\n",
    "            conInfo = {'ws': con_coeffs, 'f': con_f, 'df': con_df} # convergence information\n",
    "\n",
    "\n",
    "        return {\"w\": self.w[:self.w.shape[0] - 1], \"b\": self.w[self.w.shape[0] - 1]}, conInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
