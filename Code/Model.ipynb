{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class SSVM:\n",
    "    def __init__(self, A_pos, A_neg, C, w0, b0, delta, beta, tol=1e-5, convergSpeed=1e-8, maxIter=100, alpha=1000):\n",
    "\n",
    "        self.tol = tol\n",
    "        self.convergSpeed = convergSpeed\n",
    "        self.C = C\n",
    "        self.maxIter = maxIter\n",
    "        self.alpha = alpha # p parameter\n",
    "\n",
    "        # loss function parameters\n",
    "        self.delta = delta # -1 -> 1\n",
    "        self.beta = beta # -0.5 -> 0.5 [apart from -0.5beta -0.5 delta]\n",
    "\n",
    "        try:\n",
    "            self.A = np.vstack([np.hstack([A_pos, -np.ones([A_pos.shape[0], 1])]),\n",
    "                                np.hstack([-A_neg, np.ones([A_neg.shape[0], 1])]),])\n",
    "            self.w = np.vstack((w0, b0))\n",
    "        except:\n",
    "            print(\"\\n===Error in SSVM-init : the dimension of w, b, A_pos, A_neg not agree===\")\n",
    "            sys.exit(1)\n",
    "\n",
    "\n",
    "    def objf(self, w):\n",
    "        '''Evaluate the function value:\n",
    "            w = vector in SVM\n",
    "            Output:\n",
    "            return = value\n",
    "        '''\n",
    "\n",
    "        try:\n",
    "            margin_adj_loss = self.beta * np.exp(-self.delta * np.abs(self.A.dot(w))) \n",
    "            x =  np.ones((self.A.shape[0], 1)) - self.A.dot(w) + margin_adj_loss\n",
    "        except:\n",
    "            print(\"\\n===Error in SSVM-objf : loss function error===\")\n",
    "            sys.exit(1)\n",
    "        try:\n",
    "            # temp = (np.ones((self.A.shape[0], 1)) - self.A.dot(w))\n",
    "            # v = np.maximum(temp, 0)\n",
    "            # x = (np.ones((self.A.shape[0], 1)) - self.A.dot(w))\n",
    "            v = x + 1/self.alpha * np.log(1 + np.exp(-self.alpha * x))\n",
    "            # hinge loss + regularization\n",
    "            return 0.5 * (v.T.dot(v) + w.T.dot(w) / self.C)\n",
    "        except TypeError:\n",
    "            print(\"\\n===Error in SSVM-objf : type of parameter are not the same===\")\n",
    "            sys.exit(1)\n",
    "\n",
    "\n",
    "    def armijo(self, w, p_k, gap, obj1):\n",
    "        '''\n",
    "        Avoid the local maximum(minimum) in Newton method:\n",
    "        \n",
    "        w = current point\n",
    "        p_k = newton direction\n",
    "        gap = defined in ssvm code\n",
    "        obj1 = the object function value of current point\n",
    "        \n",
    "        Output:\n",
    "        stepsize = stepsize for Newton method\n",
    "        '''\n",
    "        diff = 0\n",
    "        stepsize = 0.5  # initial size\n",
    "        count = 1\n",
    "        try:\n",
    "            # Armijo: new function <= old function value - fraction * step length * directional derivative\n",
    "            # Wolfe sufficient decrease condition similar/same to this\n",
    "\n",
    "            # gap = p_k.T.dot(gradz). stopping criterion - If gap approaches zero, \n",
    "            # Newton direction aligns with the steepest descent direction, so small or negative gap means current solution \n",
    "            # close to the optimum, or the step might be too large if gap is negative.\n",
    "            # 0.05 = c1 in functions\n",
    "            while diff < -0.05 * stepsize * gap:\n",
    "                stepsize = 0.5 * stepsize # lambda = max{1,0.5,0.25...} as in paper\n",
    "                w2 = w + stepsize * p_k # (w^i+1, gamma^i+1) = (w_i, gamma_i) + step * direction_i\n",
    "                obj2 = self.objf(w2)\n",
    "                diff = obj1 - obj2 # f(w_i, gamma_i) - f(w^i+1, gamma^i+1) >= -delta * step * grad_f(w_i, gamma_i) * direction_i\n",
    "                \n",
    "                count = count + 1\n",
    "                if count > 20:\n",
    "                    break\n",
    "\n",
    "        except TypeError:\n",
    "            print(\"\\n===Error in SSVM-armijo : type of variables are not the same===\")\n",
    "            sys.exit(1)\n",
    "        except ValueError:\n",
    "            print(\"\\n===Error in SSVM-armijo : value of variables are not correct===\")\n",
    "\n",
    "        return stepsize\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        e = np.ones((self.A.shape[0], 1))\n",
    "        stopCond = 1\n",
    "        iter = 0\n",
    "        # info = {'ws': [self.w]}\n",
    "\n",
    "        while stopCond > self.tol and iter < self.maxIter:\n",
    "            iter = iter + 1\n",
    "            margin_adj_loss = self.beta * np.exp(-self.delta * np.abs(self.A.dot(self.w))) \n",
    "            d = e - np.dot(self.A, self.w) + margin_adj_loss\n",
    "            Point = d[:, 0] > 0\n",
    "\n",
    "            if Point.all == False:\n",
    "                return\n",
    "        \n",
    "            # Regularisation gradient - loss function gradient\n",
    "            gradz = self.w / self.C - self.A[Point, :].T.dot(d[Point]) \n",
    "            hessian = np.eye(self.A.shape[1]) / self.C + self.A[Point, :].T.dot(self.A[Point, :]) \n",
    "\n",
    "            del d\n",
    "            del Point\n",
    "\n",
    "            if (gradz.T.dot(gradz) / self.A.shape[1]) > self.tol:\n",
    "                try:\n",
    "                    # p_k = np.linalg.solve(-hessian, gradz) # - slow and unstable for large matrices\n",
    "\n",
    "                    # P, L, U = scipy.linalg.lu(hessian)\n",
    "                    # d2f_x_k_inv = scipy.linalg.inv(U) @ scipy.linalg.inv(L) @ P.T - doesnt work either\n",
    "\n",
    "                    # L = scipy.linalg.cholesky(hessian, lower=True)\n",
    "                    # d2f_x_k_inv = scipy.linalg.cho_solve((L, True), np.eye(hessian.shape[0])) - doesnt work\n",
    "\n",
    "\n",
    "                    # p_k = -np.dot(d2f_x_k_inv, gradz)\n",
    "\n",
    "                    # est_direction, info = scipy.sparse.linalg.cg(hessian, gradz, maxiter=10)\n",
    "                    # p_k = np.mean(est_direction)\n",
    "                    # print(p_k.shape)\n",
    "\n",
    "                    d2f_x_k_inv = np.linalg.inv(hessian) \n",
    "                    p_k = -np.dot(d2f_x_k_inv, gradz)\n",
    "                except:\n",
    "                    print(\"\\n===Error in SSVM-train : inverse of hessian error===\")\n",
    "                    p_k = np.zeros(self.w.shape)\n",
    "                del hessian\n",
    "\n",
    "                obj1 = self.objf(self.w)\n",
    "                w1 = self.w + p_k\n",
    "                obj2 = self.objf(w1)\n",
    "\n",
    "                #Â CHANGE - WILL TAKE TOO LONG TO CONVERGE\n",
    "                if (obj1 - obj2) <= self.convergSpeed: \n",
    "                    # Use the Armijo's rule\n",
    "                    try:\n",
    "                        # gap = p_k.T.dot(gradz). stopping criterion - If gap approaches zero, \n",
    "                        # Newton direction aligns with the steepest descent direction,so small or \n",
    "                        # negative gap means current solution close to the optimum, or the step might be too large if gap is negative.\n",
    "                        gap = p_k.T.dot(gradz)\n",
    "                    except:\n",
    "                        print(\n",
    "                            \"\\n===Error in SSVM-train : the dimesion of z and gradz not agree===\"\n",
    "                        )\n",
    "                        sys.exit(1)\n",
    "                    # Find the step size & Update to the new point\n",
    "                    stepsize = self.armijo(self.w, p_k, gap, obj1)\n",
    "                    self.w = self.w + stepsize * p_k\n",
    "                else:\n",
    "                    # Use the Newton method\n",
    "                    self.w = w1\n",
    "\n",
    "                try:\n",
    "                    stopCond = np.linalg.norm(p_k) #2-norm\n",
    "                except:\n",
    "                    print(\"\\n===Error in SSVM-train : 2norm of z error===\")\n",
    "                    sys.exit(1)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "\n",
    "        return {\"w\": self.w[:self.w.shape[0] - 1], \"b\": self.w[self.w.shape[0] - 1]} #, info\n",
    "\n",
    "\n",
    "    def convergence_history(self, info, xMin, x0, p, H=None):\n",
    "\n",
    "        # convert info['xs'] to numpy array\n",
    "        arr = np.zeros(shape=(2,len(info['xs'])))\n",
    "        for i in range(len(info['xs'])):\n",
    "            x = info['xs'][i][0]\n",
    "            y = info['xs'][i][1]\n",
    "            arr[0][i] = x\n",
    "            arr[1][i] = y\n",
    "        \n",
    "        if xMin is None:\n",
    "            xMin = (arr[0][-1], arr[1][-1])\n",
    "        \n",
    "        shape = arr.shape[1]\n",
    "        if p == 'M':\n",
    "            p = 2\n",
    "            if H is not None:\n",
    "                M = H\n",
    "            else:\n",
    "                M = self.d2f(xMin)  # M is the Hessian at the solution, M has to be symmetric positive definite\n",
    "            \n",
    "            # Convergence of iterates: || x_k - xMin ||_M\n",
    "            err = info['xs'] - np.tile(xMin, (2, shape))            \n",
    "            # err = xs - np.tile(x_min[:, np.newaxis], (1, xs.shape[1]))\n",
    "            con_x = [np.sqrt(np.dot(err[k].T, M.dot(err[k]))) for k in range(shape)]\n",
    "        else:\n",
    "            # Convergence of iterates: || x_k - xMin ||_p\n",
    "            # print((arr - np.array([xMin[0], xMin[1]]*shape).reshape(2,shape)).shape)\n",
    "            con_x = np.sum(np.abs(arr - np.array([xMin[0], xMin[1]]*shape).reshape(2,shape))**p, axis=0)**(1/p)\n",
    "\n",
    "        if self.f is not None:\n",
    "            # Convergence of function values: f(x_k) - f(xMin)\n",
    "            con_f = [self.f([arr[0,k], arr[1,k]]) - self.f(xMin) for k in range(shape)]\n",
    "\n",
    "            # Convergence of gradient: || f(x_k)||_p\n",
    "            con_df = [np.sum(np.abs(self.df([arr[0,k], arr[1,k]]))**p)**(1/p) for k in range(shape)]\n",
    "        else:\n",
    "            con_f = []\n",
    "            con_df = []\n",
    "\n",
    "        conInfo = {'x': con_x, 'f': con_f, 'df': con_df} # convergence information\n",
    "        \n",
    "        return conInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
