{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96bd28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from urllib.request import urlretrieve\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b1a57d",
   "metadata": {},
   "source": [
    "### Descent line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421ea12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def descentLineSearch(F, descent, ls, alpha0, x0, tol, maxIter, stopType):\n",
    "    \"\"\"\n",
    "    Wrapper function executing descent with line search (includes inexact Newton;\n",
    "    stopping with Newton decrement and grad)\n",
    "\n",
    "    Parameters:\n",
    "    F (dict): Dictionary with fields:\n",
    "        - f: function handler\n",
    "        - df: gradient handler\n",
    "        - d2f: Hessian handler\n",
    "    descent (str): Specifies descent direction {'steepest', 'newton', 'newton-cg', 'bfgs'}\n",
    "    ls (function): Specifies the line search to apply\n",
    "    alpha0 (float): Initial step length\n",
    "    x0 (numpy.ndarray): Initial iterate\n",
    "    tol (float): Stopping condition on minimal allowed step\n",
    "    maxIter (int): Maximum number of iterations\n",
    "    stopType (str): Stopping criteria choose from {'step', 'grad', 'dQx', 'NWdec'}\n",
    "\n",
    "    Returns:\n",
    "    xMin (numpy.ndarray): Minimum\n",
    "    fMin (float): Value of f at the minimum\n",
    "    nIter (int): Number of iterations\n",
    "    info (dict): Dictionary with information about the iteration\n",
    "        - xs: iterate history\n",
    "        - alphas: step lengths history\n",
    "        - NWdecs: Newton decrement history\n",
    "        - Hk: Dictionary with Hessian approximation history (optional)\n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    nIter = 0\n",
    "    x_k = x0\n",
    "    info = {'xs': [x0], 'alphas': [alpha0], 'NWdecs': [], 'stopType': stopType}\n",
    "    stopCond = False\n",
    "\n",
    "    # Loop until convergence or maximum number of iterations\n",
    "    while not stopCond and nIter <= maxIter:\n",
    "        # Increment iterations\n",
    "        nIter += 1\n",
    "\n",
    "        # Compute descent direction\n",
    "        if descent.lower() == 'steepest':\n",
    "            p_k = -F['df'](x_k)  # steepest descent direction\n",
    "        elif descent.lower() == 'newton':\n",
    "            p_k = -np.linalg.solve(F['d2f'](x_k), F['df'](x_k))  # Newton direction\n",
    "            if np.dot(p_k, F['df'](x_k)) > 0:\n",
    "                print(f\"Newton iteration {nIter}, produced not a descent direction\")\n",
    "        elif descent.lower() == 'newton-cg':\n",
    "            # Conjugate gradient method\n",
    "            df_k = F['df'](x_k)  # gradient\n",
    "            B_k = F['d2f'](x_k)  # hessian\n",
    "            eps_k = min(0.5, np.sqrt(np.linalg.norm(df_k))) * np.linalg.norm(df_k)\n",
    "            z_j = np.zeros_like(df_k)\n",
    "            r_j = df_k\n",
    "            d_j = -df_k\n",
    "            stopCondCG = False\n",
    "            maxIterCG = 200  # maxIter\n",
    "            nIterCG = 0\n",
    "            while not stopCondCG and nIterCG <= maxIterCG:\n",
    "                if np.dot(d_j, np.dot(B_k, d_j)) <= 0:\n",
    "                    if nIterCG == 0:\n",
    "                        p_k = d_j\n",
    "                    else:\n",
    "                        p_k = z_j\n",
    "                    stopCondCG = True\n",
    "                    eps_k = 0\n",
    "                norm_r_j = np.dot(r_j, r_j)\n",
    "                a_j = norm_r_j / np.dot(d_j, np.dot(B_k, d_j))\n",
    "                z_j = z_j + a_j * d_j\n",
    "                r_j = r_j + a_j * np.dot(B_k, d_j)\n",
    "                if np.sqrt(np.dot(r_j, r_j)) < eps_k or nIterCG == maxIterCG:\n",
    "                    stopCondCG = True\n",
    "                    p_k = z_j\n",
    "                b_j = np.dot(r_j, r_j) / norm_r_j\n",
    "                d_j = -r_j + b_j * d_j\n",
    "                nIterCG += 1\n",
    "        else:\n",
    "            raise ValueError(\"Invalid descent method specified.\")\n",
    "\n",
    "        # Call line search given by handle ls for computing step length\n",
    "        alpha_k = ls(x_k, p_k, alpha0)\n",
    "\n",
    "        # Newton decrement\n",
    "        NWdec = alpha_k**2 * np.dot(p_k, np.dot(F['d2f'](x_k), p_k)) / 2\n",
    "        info['NWdecs'].append(NWdec)\n",
    "\n",
    "        # Update x_k and f_k\n",
    "        x_k_1 = x_k\n",
    "        x_k = x_k + alpha_k * p_k\n",
    "\n",
    "        # Store iteration info\n",
    "        info['xs'].append(x_k)\n",
    "        info['alphas'].append(alpha_k)\n",
    "\n",
    "        if stopType == 'step':\n",
    "            # Compute relative step length\n",
    "            normStep = np.linalg.norm(x_k - x_k_1) / np.linalg.norm(x_k_1)\n",
    "            stopCond = (normStep < tol)\n",
    "        elif stopType == 'grad':\n",
    "            stopCond = (np.linalg.norm(F['df'](x_k), np.inf) < tol * (1 + abs(F['f'](x_k))))\n",
    "        elif stopType == 'dQx':\n",
    "            stopCond = (np.linalg.norm(F['df'](x_k), 2) < tol)\n",
    "        elif stopType == 'NWdec':\n",
    "            stopCond = NWdec < tol\n",
    "\n",
    "    # Assign output values\n",
    "    xMin = x_k\n",
    "    fMin = F['f'](x_k)\n",
    "    \n",
    "    return xMin, fMin, nIter, info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2362ab",
   "metadata": {},
   "source": [
    "### Interior Point Method PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8507464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from InteriorPoint.interiorPoint_PrimalDual import interiorPoint_PrimalDual\n",
    "from InteriorPoint.mergeConstraints import mergeConstraints\n",
    "from tutorial1.solutions.m.convergenceHistory import visualizeConvergence\n",
    "from tutorial3.solutions.m.quartic1 import quartic1\n",
    "\n",
    "# Close all figures and clear workspace\n",
    "plt.close('all')\n",
    "np.random.seed(0)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Objective function\n",
    "F = quartic1()\n",
    "\n",
    "# Constraints\n",
    "# Inequality constraint (linear): y < x - 1 <=> -x + y + 1 < 0\n",
    "constraintL = {'f': lambda x: -x[0] + x[1] + 1,\n",
    "               'df': lambda x: np.array([-1, 1]),\n",
    "               'd2f': lambda x: np.zeros((2, 2))}\n",
    "ineqConstraints = [constraintL]  # {constraintC}, {constraintL}, {constraintC, constraintL},\n",
    "lambda0 = np.ones(len(ineqConstraints))  # Ensure: lambda > 0\n",
    "\n",
    "# Equality constraint\n",
    "eqConstraint = {'A': np.array([[-2, -1]]), 'b': -1/2}\n",
    "nu0 = 1\n",
    "\n",
    "# Initialization\n",
    "x0 = np.array([1, -2*1 + 1/2])\n",
    "\n",
    "# Parameters\n",
    "mu = 10  # in (3, 100);\n",
    "tol = 1e-12\n",
    "maxIter = 200\n",
    "optsBT = {'maxIter': 30, 'alpha': 0.1, 'beta': 0.8}\n",
    "\n",
    "# Minimization\n",
    "ineqConstraint = mergeConstraints(ineqConstraints)\n",
    "xPD, fPD, tPD, nPD, infoPD = interiorPoint_PrimalDual(F, ineqConstraint, eqConstraint, x0, lambda0, nu0, mu, tol, tol, maxIter, optsBT)\n",
    "print('xPD =', xPD)\n",
    "\n",
    "# Visualize iterates\n",
    "x = np.arange(-8, 9, 0.1) / 4\n",
    "y = np.arange(-8, 9, 0.1) / 4\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = F['f'](X, Y)\n",
    "\n",
    "visualizeConvergence(infoPD, X, Y, Z, 'final')\n",
    "plt.title('Primal-Dual')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(x, x - 1, '-.k', linewidth=2)\n",
    "plt.plot(x, -2*x + 1/2, '--k', linewidth=2)\n",
    "plt.legend(['f(x)', '$x_k$', '$y \\leq x-1$', '$y = -2x+1/2$'], loc='upper right', fontsize='large')\n",
    "plt.axis([x[0], x[-1], y[0], y[-1]])\n",
    "plt.grid(True)\n",
    "\n",
    "# Primal-Dual: residuals, surrogate gap\n",
    "plt.figure()\n",
    "plt.semilogy(infoPD['r_dual'], '-ob', markersize=5)\n",
    "plt.semilogy(infoPD['r_cent'], '-+k', markersize=5)\n",
    "plt.semilogy(infoPD['r_prim'] + np.finfo(float).eps, '--xr', markersize=5)\n",
    "plt.semilogy(infoPD['surgap'], '--sg', markersize=5)\n",
    "plt.legend(['Dual residual', 'Centering residual', 'Primal residual', 'Surrogate gap'])\n",
    "plt.title('Primal-Dual')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6392f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def realLog(x):\n",
    "    if x <= 0:\n",
    "        ret = -np.inf\n",
    "    else:\n",
    "        ret = np.log(x)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf17a5",
   "metadata": {},
   "source": [
    "### Interior Point Barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from descentLineSearch import descentLineSearch\n",
    "from feasibleNewton import feasibleNewton\n",
    "from backtracking import backtracking\n",
    "\n",
    "def interiorPoint_Barrier(F, phi, eqConstraint, x0, t, mu, tol, maxIter):\n",
    "    \"\"\"\n",
    "    Barrier method for inequality constraints.\n",
    "\n",
    "    Solvers problem with inequality constraints. Handling of equality constraints is not implemented.\n",
    "    Calls backtracking.m and descentLineSearch.m (expects Newton decrement stopping criteria)\n",
    "\n",
    "    Parameters:\n",
    "    F (dict): Dictionary with fields:\n",
    "        - f: function to minimize\n",
    "        - df: gradient of function\n",
    "        - d2f: Hessian of function\n",
    "    phi (dict): Structure with fields:\n",
    "        - f: barrier function\n",
    "        - df: gradient of barrier function\n",
    "        - d2f: Hessian of barrier function\n",
    "    eqConstraint (dict): Linear equality constraint A x = b as structure with fields:\n",
    "        - A: matrix for the equality constraints\n",
    "        - b: vector for the equality constraints\n",
    "        Set [] if no constraints\n",
    "    x0 (numpy.ndarray): Initial iterate\n",
    "    t (float): Barrier weight\n",
    "    mu (float): Factor to increase t <- mu*t\n",
    "    tol (float): Tolerance for the (duality gap ~ m/t) assumed scaled with 1/m, m #inequality constraints.\n",
    "    maxIter (int): Maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "    xMin (numpy.ndarray): Minimum\n",
    "    fMin (float): Value of f at the minimum\n",
    "    t (float): Final barrier weight\n",
    "    nIter (int): Number of iterations\n",
    "    infoBarrier (dict): Dictionary with information about the iteration\n",
    "        - xs: iterate history for x\n",
    "        - xs_nw: iterate history for y\n",
    "        - inIter: number of interior iterations\n",
    "        - dGap: duality gap (scaled) 1/t\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    nIter = 0\n",
    "    stopCond = False\n",
    "    x_k = x0\n",
    "\n",
    "    # Parameters for centering step\n",
    "    alpha0 = 1\n",
    "    opts = {'c1': 1e-4, 'c2': 0.9, 'rho': 0.5}\n",
    "    tolNewton = 1e-12\n",
    "    maxIterNewton = 100\n",
    "\n",
    "    infoBarrier = {'xs': [x0], 'xs_nw': [x0], 'alphas_nw': [alpha0], 'fs': [F['f'](x0)], 'inIter': [0], 'dGap': [1/t], 'NWdecs': []}\n",
    "\n",
    "    # Outer iteration\n",
    "    while not stopCond and nIter < maxIter:\n",
    "        print('Iteration', nIter)\n",
    "        # Create function handler for centering step\n",
    "        G = {'f': lambda x: t*F['f'](x) + phi['f'](x),\n",
    "             'df': lambda x: t*F['df'](x) + phi['df'](x),\n",
    "             'd2f': lambda x: t*F['d2f'](x) + phi['d2f'](x)}\n",
    "\n",
    "        # Line search function\n",
    "        lsFun = lambda x_k, p_k, alpha0: backtracking(G, x_k, p_k, alpha0, opts)\n",
    "\n",
    "        # Centering step - inner iteration, with Newton decrement stopping\n",
    "        if not eqConstraint:\n",
    "            x_k, f_k, nIterNW, infoNW = descentLineSearch(G, None, lsFun, alpha0, x_k, tolNewton, maxIterNewton, 'NWdec')\n",
    "        else:\n",
    "            x_k, f_k, nIterNW, infoNW = feasibleNewton(G, eqConstraint, lsFun, alpha0, x_k, tolNewton, maxIterNewton, 'NWdec')\n",
    "\n",
    "        # Check stopping condition (m/t)\n",
    "        if 1/t < tol:\n",
    "            stopCond = True\n",
    "\n",
    "        # Increase t\n",
    "        t = mu * t\n",
    "\n",
    "        # Store info\n",
    "        infoBarrier['xs'].append(x_k)\n",
    "        infoBarrier['xs_nw'].append(infoNW['xs'][:, 1:])\n",
    "        infoBarrier['alphas_nw'].append(infoNW['alphas'][1:])\n",
    "        infoBarrier['NWdecs'].append(infoNW['NWdecs'])\n",
    "        infoBarrier['fs'].append(f_k)\n",
    "        infoBarrier['inIter'].append(nIterNW)\n",
    "        infoBarrier['dGap'].append(1/t)\n",
    "\n",
    "        # Increment number of iterations\n",
    "        nIter += 1\n",
    "\n",
    "    # Assign values\n",
    "    xMin = x_k\n",
    "    fMin = F['f'](x_k)\n",
    "\n",
    "    return xMin, fMin, t, nIter, infoBarrier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e92d76",
   "metadata": {},
   "source": [
    "### Interior Point Primal Duel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bbb3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interiorPoint_PrimalDual(F, ineqConstraint, eqConstraint, x0, lambda0, nu0, mu, tol, tolFeas, maxIter, opts):\n",
    "    \"\"\"\n",
    "    Primal-dual interior point method for inequality constraints.\n",
    "\n",
    "    Solves problem with inequality constraints (mandatory) and equality\n",
    "    constraints (optional) using primal-dual interior point method.\n",
    "\n",
    "    Parameters:\n",
    "    F (dict): Dictionary with fields:\n",
    "        - f: function to minimise\n",
    "        - df: gradient of function \n",
    "        - d2f: Hessian of function \n",
    "    ineqConstraint (dict): Structure with fields:\n",
    "        - f: vectorial linear function that sets the inequality constraints\n",
    "        - df: Jacobian of the vectorial function f\n",
    "    eqConstraint (dict): Structure with fields:\n",
    "        - A: matrix for the equality constraints\n",
    "        - b: vector for the equality constraints\n",
    "    x0 (numpy.ndarray): Initial iterate\n",
    "    lambda0 (numpy.ndarray): Initial inequality Lagrange parameters\n",
    "    nu0 (numpy.ndarray): Initial equality Lagrange parameters\n",
    "    mu (float): Factor for increasing t <- t*mu\n",
    "    tol (float): Tolerance for the surrogate duality gap \n",
    "    tolFeas (float): Tolerance for primal and dual residuals\n",
    "    maxIter (int): Maximum number of iterations\n",
    "    opts (dict): Options for backtracking\n",
    "\n",
    "    Returns:\n",
    "    xMin (numpy.ndarray): Minimum\n",
    "    fMin (float): Value of f at the minimum\n",
    "    t (float): Final barrier weight\n",
    "    nIter (int): Number of iterations\n",
    "    infoPD (dict): Dictionary with information about the iteration \n",
    "        - xs: iterate history for x \n",
    "        - lambdas: iterate history for lambda\n",
    "        - nus: iterate history for nu\n",
    "        - r_dual: dual residual history\n",
    "        - r_cent: centering residual history\n",
    "        - r_prim: primal residual history\n",
    "        - surgap: surrogate gap history\n",
    "        - s: step length history\n",
    "        - t: barrier weight history\n",
    "        - BTs: backtracking iterations history\n",
    "        - s_max: maximum step length history\n",
    "    \"\"\"\n",
    "    # Dimensions\n",
    "    n = len(x0)\n",
    "    # Check if the inequality constraints are set\n",
    "    if 'f' not in ineqConstraint:\n",
    "        ineqConstraint['f'] = lambda x: -np.inf\n",
    "    if 'df' not in ineqConstraint:\n",
    "        ineqConstraint['df'] = lambda x: np.zeros((1, n))\n",
    "    m = len(lambda0)\n",
    "    # Check if the equality constraints are set\n",
    "    if 'A' not in eqConstraint:\n",
    "        eqConstraint['A'] = np.zeros((0, n))\n",
    "    if 'b' not in eqConstraint:\n",
    "        eqConstraint['b'] = np.zeros((0, 1))\n",
    "    nEq = eqConstraint['A'].shape[0]\n",
    "\n",
    "    # Initilize\n",
    "    nIter = 0\n",
    "    stopCond = False\n",
    "    x_k = x0\n",
    "    l_k = lambda0\n",
    "    n_k = nu0\n",
    "    infoPD = {'xs': [x0], 'lambdas': [lambda0], 'nus': [nu0], 'r_dual': [], 'r_cent': [], 'r_prim': [], 'surgap': [],\n",
    "              's': [], 't': [], 'BTs': [], 's_max': []}\n",
    "\n",
    "    # Define residual function\n",
    "    def r_dual(t, x, l, n):\n",
    "        return F['df'](x) + np.dot(ineqConstraint['df'](x).T, l) + np.dot(eqConstraint['A'].T, n)\n",
    "\n",
    "    def r_cent(t, x, l, n):\n",
    "        return -np.diag(l).dot(ineqConstraint['f'](x)) - np.ones((m, 1)) / t\n",
    "\n",
    "    def r_prim(t, x, l, n):\n",
    "        return eqConstraint['A'].dot(x) - eqConstraint['b']\n",
    "\n",
    "    res = lambda t, x, l, n: np.vstack((r_dual(t, x, l, n), r_cent(t, x, l, n), r_prim(t, x, l, n)))\n",
    "\n",
    "    # Surrogate gap\n",
    "    eta = -(ineqConstraint['f'](x0)).dot(lambda0)\n",
    "    t = mu * m / eta\n",
    "\n",
    "    # Loop\n",
    "    while not stopCond and nIter < maxIter:\n",
    "        print('Iteration', nIter)\n",
    "\n",
    "        # Compute residual\n",
    "        res_k = res(t, x_k, l_k, n_k)\n",
    "\n",
    "        # Find direction\n",
    "        hessIneqConstraints_x_k = sum(l_k[j] * ineqConstraint['d2f'](x_k)[:, :, j] for j in range(m))\n",
    "\n",
    "        deltaY = -np.linalg.solve(\n",
    "            np.block([[F['d2f'](x_k) + hessIneqConstraints_x_k, ineqConstraint['df'](x_k).T, eqConstraint['A'].T],\n",
    "                      [-np.diag(l_k).dot(ineqConstraint['df'](x_k)), -np.diag(ineqConstraint['f'](x_k)).flatten(),\n",
    "                       np.zeros((m, nEq))],\n",
    "                      [eqConstraint['A'], np.zeros((nEq, m)), np.zeros((nEq, nEq))]]), res_k)\n",
    "\n",
    "        # Backtracking line search adapted to PD\n",
    "        deltaX = deltaY[:n]  # delta x\n",
    "        deltaL = deltaY[n:n + m]  # delta lambda\n",
    "        deltaN = deltaY[n + m:]  # delta nu\n",
    "        # Ensure: lambda + s * deltaL >= 0\n",
    "        l_k_neg = l_k[deltaL < 0] / deltaL[deltaL < 0]  # identify negative deltaL\n",
    "        if l_k_neg.size > 0:\n",
    "            # ensure l_k + s*deltaL >= 0 (only needed for deltaL < 0)\n",
    "            # equivalent to s <= -l_k/*deltaL and (-l_k/*deltaL) > 0\n",
    "            s_max = min(1, 0.99 * (-l_k[deltaL < 0] / deltaL[deltaL < 0]))  # *0.99 to ensure lambda > 0\n",
    "        else:\n",
    "            s_max = 1\n",
    "        # multiply with rho in (0,1)\n",
    "        s = s_max\n",
    "        # Ensure:\n",
    "        #  all ineqConstraint.f(x_k + s*deltaX)) < 0\n",
    "        #  sufficient reduction of ||r||: ||r(x_k + s*deltaX, l_k + s*deltaL, n_k + s*deltaN) <= (1-alpha*s) r(x_k, l_k, n_k) ||\n",
    "        nIterBT = 0\n",
    "        stopCondBT = False\n",
    "        while not stopCondBT and nIterBT < opts['maxIter']:\n",
    "            # reduce 's' until \n",
    "            #  all ineqConstraint.f(x_k + s*deltaX)) < 0  &\n",
    "            #  ||r(x_k + s*deltaX, l_k + s*deltaL, n_k + s*deltaN) <= (1-alpha*s) r(x_k, l_k, n_k) ||\n",
    "            # are satisfied\n",
    "            if max(ineqConstraint['f'](x_k + s * deltaX)) < 0 and \\\n",
    "                    np.linalg.norm(res(t, x_k + s * deltaX, l_k + s * deltaL, n_k + s * deltaN)) <= \\\n",
    "                    (1 - opts['alpha'] * s) * np.linalg.norm(res_k):\n",
    "                stopCondBT = True\n",
    "            else:\n",
    "                s = opts['beta'] * s\n",
    "                nIterBT += 1\n",
    "\n",
    "        # Update point\n",
    "        x_k = x_k + s * deltaX\n",
    "        l_k = l_k + s * deltaL\n",
    "        n_k = n_k + s * deltaN\n",
    "\n",
    "        # Surrogate gap\n",
    "        eta = -(ineqConstraint['f'](x_k)).dot(l_k)\n",
    "        t = mu * m / eta\n",
    "\n",
    "        # Stopping criteria\n",
    "        # Check if all residuals are below the tolerances\n",
    "        if eta < tol and \\\n",
    "                np.linalg.norm(r_dual(t, x_k, l_k, n_k)) <= tolFeas and \\\n",
    "                np.linalg.norm(r_prim(t, x_k, l_k, n_k)) <= tolFeas:\n",
    "            stopCond = True\n",
    "\n",
    "        # Harvest info\n",
    "        infoPD['xs'].append(x_k)\n",
    "        infoPD['lambdas'].append(l_k)\n",
    "        infoPD['nus'].append(n_k)\n",
    "        infoPD['r_dual'].append(np.linalg.norm(r_dual(t, x_k, l_k, n_k)))\n",
    "        infoPD['r_cent'].append(np.linalg.norm(r_cent(t, x_k, l_k, n_k)))\n",
    "        infoPD['r_prim'].append(np.linalg.norm(r_prim(t, x_k, l_k, n_k)))\n",
    "        infoPD['surgap'].append(eta)\n",
    "        infoPD['s'].append(s)\n",
    "        infoPD['t'].append(t)\n",
    "        infoPD['BTs'].append(nIterBT)\n",
    "        infoPD['s_max'].append(s_max)\n",
    "\n",
    "        # Increment number of iterations\n",
    "        nIter += 1\n",
    "\n",
    "    # Assign values\n",
    "    xMin = x_k\n",
    "    fMin = F['f'](x_k)\n",
    "\n",
    "    return xMin, fMin, t, nIter, infoPD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca32771",
   "metadata": {},
   "source": [
    "### Merge Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31120bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeConstraints(ineqConstraints):\n",
    "    \"\"\"\n",
    "    Merges together multiple inequality constraints into multidimensional\n",
    "    constraint as used in interiorPoint_PrimalDual.m\n",
    "\n",
    "    Parameters:\n",
    "    ineqConstraints (list): List containing individual constraints as dictionaries each comprising fields:\n",
    "        constraint['f']: constraint function constraint['f'](x) < 0\n",
    "        constraint['df']: gradient of constraint function (as a row)\n",
    "        constraint['d2f']: Hessian of constraint function \n",
    "\n",
    "    Returns:\n",
    "    constraint (dict): Multidimensional constraint as a dictionary with fields:\n",
    "        constraint['f']: column vector with constraint functions constraint['f'](x) < 0 as elements\n",
    "        constraint['df']: matrix with gradients of constraint functions as rows\n",
    "        constraint['d2f']: 3 dimensional tensor with Hessians of constraint functions indexed by the third component\n",
    "    \"\"\"\n",
    "    # Define functions for constraint, gradient, and Hessian\n",
    "    def constraint(x):\n",
    "        return np.array([constraint_dict['f'](x) for constraint_dict in ineqConstraints])\n",
    "\n",
    "    def gradConstraint(x):\n",
    "        return np.vstack([constraint_dict['df'](x) for constraint_dict in ineqConstraints])\n",
    "\n",
    "    def HessianConstraint(x):\n",
    "        return np.dstack([constraint_dict['d2f'](x) for constraint_dict in ineqConstraints])\n",
    "\n",
    "    # Construct the merged constraint dictionary\n",
    "    ineqConstraint = {\n",
    "        'f': constraint,\n",
    "        'df': gradConstraint,\n",
    "        'd2f': HessianConstraint\n",
    "    }\n",
    "\n",
    "    return ineqConstraint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6270dd7",
   "metadata": {},
   "source": [
    "### Create Barrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d35c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBarrier(ineqConstraints):\n",
    "    \"\"\"\n",
    "    Creates a barrier function from R^n -> R\n",
    "\n",
    "    Parameters:\n",
    "    ineqConstraints (list): List containing individual constraints as dictionaries each comprising fields:\n",
    "        constraint['f']: constraint function constraint['f'](x) < 0\n",
    "        constraint['df']: gradient of constraint function (as a row)\n",
    "        constraint['d2f']: Hessian of constraint function \n",
    "\n",
    "    Returns:\n",
    "    phi (dict): Barrier function with fields:\n",
    "        phi['f']: barrier function\n",
    "        phi['df']: gradient of barrier function\n",
    "        phi['d2f']: Hessian of barrier function\n",
    "    \"\"\"\n",
    "    # Define functions for barrier, gradient, and Hessian\n",
    "    def barrier(x):\n",
    "        f = 0\n",
    "        for constraint_dict in ineqConstraints:\n",
    "            f += -np.real(np.log(-constraint_dict['f'](x)))\n",
    "        return f\n",
    "\n",
    "    def gradBarrier(x):\n",
    "        df = np.zeros_like(x)\n",
    "        for constraint_dict in ineqConstraints:\n",
    "            df += -1 / constraint_dict['f'](x) * constraint_dict['df'](x).T\n",
    "        return df\n",
    "\n",
    "    def HessianBarrier(x):\n",
    "        d2f = np.zeros((len(x), len(x)))\n",
    "        for constraint_dict in ineqConstraints:\n",
    "            d2f += 1 / (constraint_dict['f'](x) ** 2) * (constraint_dict['df'](x) @ constraint_dict['df'](x).T) + \\\n",
    "                   -1 / constraint_dict['f'](x) * constraint_dict['d2f'](x)\n",
    "        return d2f\n",
    "\n",
    "    # Construct the barrier function dictionary\n",
    "    phi = {\n",
    "        'f': barrier,\n",
    "        'df': gradBarrier,\n",
    "        'd2f': HessianBarrier\n",
    "    }\n",
    "\n",
    "    return phi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1147f4",
   "metadata": {},
   "source": [
    "### Feasible Newton?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd9df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feasibleNewton(F, eqConstraint, ls, alpha0, x0, tol, maxIter, stopType):\n",
    "    \"\"\"\n",
    "    Minimizes convex function subject to linear equality constraints.\n",
    "\n",
    "    Parameters:\n",
    "    F (dict): Dictionary with fields:\n",
    "        - f: function handler\n",
    "        - df: gradient handler\n",
    "        - d2f: Hessian handler\n",
    "    eqConstraint (dict): Dictionary with fields:\n",
    "        - A: matrix for the equality constraints\n",
    "        - b: vector for the equality constraints\n",
    "    ls (function): Specifies the line search to apply\n",
    "    alpha0 (float): Initial step length\n",
    "    x0 (numpy.ndarray): Feasible initialization\n",
    "    tol (float): Stopping condition on minimal allowed step\n",
    "                norm(x_k - x_k_1)/norm(x_k) < tol;\n",
    "    maxIter (int): Maximum number of iterations\n",
    "    stopType (str): Stopping condition {'step', 'grad', 'dQx', 'NWdec'}\n",
    "\n",
    "    Returns:\n",
    "    xMin (numpy.ndarray): Minimum\n",
    "    fMin (float): Value of f at the minimum\n",
    "    nIter (int): Number of iterations\n",
    "    info (dict): Dictionary with information about the iteration\n",
    "        - xs: iterate history\n",
    "        - alphas: step lengths history\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    if stopType is None:\n",
    "        stopType = 'NWdec'\n",
    "\n",
    "    # Initialization\n",
    "    nIter = 0\n",
    "    x_k = x0\n",
    "    info = {'xs': [x0], 'alphas': [alpha0], 'NWdecs': [], 'stopType': stopType}\n",
    "    stopCond = False\n",
    "\n",
    "    # Sizes\n",
    "    p = eqConstraint['A'].shape[0]\n",
    "    n = len(x0)\n",
    "\n",
    "    # Loop until convergence or maximum number of iterations\n",
    "    while not stopCond and nIter <= maxIter:\n",
    "        # Increment iterations\n",
    "        nIter += 1\n",
    "\n",
    "        # Newton direction: (Delta x, w)\n",
    "        p_k = np.linalg.solve(\n",
    "            np.block([[F['d2f'](x_k), eqConstraint['A'].T],\n",
    "                      [eqConstraint['A'], np.zeros((p, p))]]),\n",
    "            np.block([[-F['df'](x_k)], [np.zeros(p)]]))\n",
    "\n",
    "        # Newton direction Delta x only\n",
    "        dx_k = p_k[:n]\n",
    "\n",
    "        # Call line search given by handle ls for computing step length\n",
    "        alpha_k = ls(x_k, dx_k, alpha0)\n",
    "\n",
    "        # Newton decrement lambda\n",
    "        NWdec = alpha_k**2 * np.dot(dx_k, np.dot(F['d2f'](x_k), dx_k)) / 2\n",
    "        info['NWdecs'].append(NWdec)\n",
    "\n",
    "        # Update x_k\n",
    "        x_k_1 = x_k\n",
    "        x_k = x_k + alpha_k * dx_k\n",
    "\n",
    "        # Store iteration info\n",
    "        info['xs'].append(x_k)\n",
    "        info['alphas'].append(alpha_k)\n",
    "\n",
    "        # Stopping condition\n",
    "        if stopType == 'step':\n",
    "            normStep = np.linalg.norm(x_k - x_k_1) / np.linalg.norm(x_k_1)\n",
    "            stopCond = normStep < tol\n",
    "        elif stopType == 'grad':\n",
    "            stopCond = np.linalg.norm(F['df'](x_k), np.inf) < tol * (1 + np.abs(F['f'](x_k)))\n",
    "        elif stopType == 'dQx':\n",
    "            stopCond = np.linalg.norm(F['df'](x_k), 2) < tol\n",
    "        elif stopType == 'NWdec':\n",
    "            stopCond = NWdec < tol\n",
    "\n",
    "    # Assign output values\n",
    "    xMin = x_k\n",
    "    fMin = F['f'](x_k)\n",
    "\n",
    "    return xMin, fMin, nIter, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d42aa4",
   "metadata": {},
   "source": [
    "## Tutorial 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238fe92",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f1269d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3 and the array at index 1 has size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/a44791/Documents/Uni/NumOptimisation/Coursework2/Code/Tutorial8_9.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/a44791/Documents/Uni/NumOptimisation/Coursework2/Code/Tutorial8_9.ipynb#X23sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m lsFun \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x_k, p_k, alpha0: backtracking({\u001b[39m'\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m: f, \u001b[39m'\u001b[39m\u001b[39mdf\u001b[39m\u001b[39m'\u001b[39m: df}, x_k, p_k, alpha0, lsOptsNewton)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/a44791/Documents/Uni/NumOptimisation/Coursework2/Code/Tutorial8_9.ipynb#X23sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39m# Solve the optimization problem\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/a44791/Documents/Uni/NumOptimisation/Coursework2/Code/Tutorial8_9.ipynb#X23sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m xMin, fMin, nIter, info \u001b[39m=\u001b[39m feasibleNewton({\u001b[39m'\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m: f, \u001b[39m'\u001b[39;49m\u001b[39mdf\u001b[39;49m\u001b[39m'\u001b[39;49m: df, \u001b[39m'\u001b[39;49m\u001b[39md2f\u001b[39;49m\u001b[39m'\u001b[39;49m: d2f}, eqConstraint, lsFun, alpha0, x0, tol, maxIter)\n",
      "\u001b[1;32m/Users/a44791/Documents/Uni/NumOptimisation/Coursework2/Code/Tutorial8_9.ipynb Cell 19\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/a44791/Documents/Uni/NumOptimisation/Coursework2/Code/Tutorial8_9.ipynb#X23sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m nIter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/a44791/Documents/Uni/NumOptimisation/Coursework2/Code/Tutorial8_9.ipynb#X23sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Newton direction\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/a44791/Documents/Uni/NumOptimisation/Coursework2/Code/Tutorial8_9.ipynb#X23sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m p_k \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39msolve(np\u001b[39m.\u001b[39;49mblock([[F[\u001b[39m'\u001b[39;49m\u001b[39md2f\u001b[39;49m\u001b[39m'\u001b[39;49m](x_k), eqConstraint[\u001b[39m'\u001b[39;49m\u001b[39mA\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mT],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/a44791/Documents/Uni/NumOptimisation/Coursework2/Code/Tutorial8_9.ipynb#X23sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m                                  [eqConstraint[\u001b[39m'\u001b[39;49m\u001b[39mA\u001b[39;49m\u001b[39m'\u001b[39;49m], np\u001b[39m.\u001b[39;49mzeros_like(eqConstraint[\u001b[39m'\u001b[39;49m\u001b[39mA\u001b[39;49m\u001b[39m'\u001b[39;49m])]]),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/a44791/Documents/Uni/NumOptimisation/Coursework2/Code/Tutorial8_9.ipynb#X23sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m                       np\u001b[39m.\u001b[39mhstack([\u001b[39m-\u001b[39mF[\u001b[39m'\u001b[39m\u001b[39mdf\u001b[39m\u001b[39m'\u001b[39m](x_k), np\u001b[39m.\u001b[39mzeros(eqConstraint[\u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/a44791/Documents/Uni/NumOptimisation/Coursework2/Code/Tutorial8_9.ipynb#X23sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Newton direction Delta x only\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/a44791/Documents/Uni/NumOptimisation/Coursework2/Code/Tutorial8_9.ipynb#X23sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m dx_k \u001b[39m=\u001b[39m p_k[:\u001b[39mlen\u001b[39m(x_k)]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/shape_base.py:872\u001b[0m, in \u001b[0;36mblock\u001b[0;34m(arrays)\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[39mreturn\u001b[39;00m _block_slicing(arrays, list_ndim, result_ndim)\n\u001b[1;32m    871\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 872\u001b[0m     \u001b[39mreturn\u001b[39;00m _block_concatenate(arrays, list_ndim, result_ndim)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/shape_base.py:916\u001b[0m, in \u001b[0;36m_block_concatenate\u001b[0;34m(arrays, list_ndim, result_ndim)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_block_concatenate\u001b[39m(arrays, list_ndim, result_ndim):\n\u001b[0;32m--> 916\u001b[0m     result \u001b[39m=\u001b[39m _block(arrays, list_ndim, result_ndim)\n\u001b[1;32m    917\u001b[0m     \u001b[39mif\u001b[39;00m list_ndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    918\u001b[0m         \u001b[39m# Catch an edge case where _block returns a view because\u001b[39;00m\n\u001b[1;32m    919\u001b[0m         \u001b[39m# `arrays` is a single numpy array and not a list of numpy arrays.\u001b[39;00m\n\u001b[1;32m    920\u001b[0m         \u001b[39m# This might copy scalars or lists twice, but this isn't a likely\u001b[39;00m\n\u001b[1;32m    921\u001b[0m         \u001b[39m# usecase for those interested in performance\u001b[39;00m\n\u001b[1;32m    922\u001b[0m         result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/core/shape_base.py:685\u001b[0m, in \u001b[0;36m_block\u001b[0;34m(arrays, max_depth, result_ndim, depth)\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[39mif\u001b[39;00m depth \u001b[39m<\u001b[39m max_depth:\n\u001b[1;32m    683\u001b[0m     arrs \u001b[39m=\u001b[39m [_block(arr, max_depth, result_ndim, depth\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    684\u001b[0m             \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 685\u001b[0m     \u001b[39mreturn\u001b[39;00m _concatenate(arrs, axis\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m(max_depth\u001b[39m-\u001b[39;49mdepth))\n\u001b[1;32m    686\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     \u001b[39m# We've 'bottomed out' - arrays is either a scalar or an array\u001b[39;00m\n\u001b[1;32m    688\u001b[0m     \u001b[39m# type(arrays) is not list\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[39mreturn\u001b[39;00m _atleast_nd(arrays, result_ndim)\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3 and the array at index 1 has size 4"
     ]
    }
   ],
   "source": [
    "def feasibleNewton(F, eqConstraint, lsFun, alpha0, x0, tol, maxIter):\n",
    "    \"\"\"\n",
    "    Minimizes convex function subject to linear equality constraints using the feasible Newton method.\n",
    "\n",
    "    Parameters:\n",
    "    F (dict): Dictionary containing function, gradient, and Hessian.\n",
    "    eqConstraint (dict): Dictionary containing equality constraints.\n",
    "    lsFun (function): Line search function.\n",
    "    alpha0 (float): Initial step length.\n",
    "    x0 (numpy.ndarray): Feasible initialization.\n",
    "    tol (float): Stopping condition on minimal allowed step.\n",
    "    maxIter (int): Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "    xMin (numpy.ndarray): Minimum point.\n",
    "    fMin (float): Minimum value of the function.\n",
    "    nIter (int): Number of iterations.\n",
    "    info (dict): Information about the iteration.\n",
    "    \"\"\"\n",
    "    # Initialization\n",
    "    nIter = 0\n",
    "    x_k = x0\n",
    "    info = {'xs': [x0], 'alphas': [alpha0], 'NWdecs': []}\n",
    "    stopCond = False\n",
    "\n",
    "    # Loop until convergence or maximum number of iterations\n",
    "    while (not stopCond and nIter <= maxIter):\n",
    "        # Increment iterations\n",
    "        nIter += 1\n",
    "\n",
    "        # Newton direction\n",
    "        p_k = np.linalg.solve(np.block([[F['d2f'](x_k), eqConstraint['A'].T],\n",
    "                                         [eqConstraint['A'], np.zeros_like(eqConstraint['A'])]]),\n",
    "                              np.hstack([-F['df'](x_k), np.zeros(eqConstraint['A'].shape[0])]))\n",
    "\n",
    "        # Newton direction Delta x only\n",
    "        dx_k = p_k[:len(x_k)]\n",
    "\n",
    "        # Call line search function\n",
    "        alpha_k = Functions.linesearch(x_k, dx_k, alpha0)\n",
    "\n",
    "        # Newton decrement\n",
    "        NWdec = alpha_k**2 * dx_k @ F['d2f'](x_k) @ dx_k / 2\n",
    "        info['NWdecs'].append(NWdec)\n",
    "\n",
    "        # Update x_k\n",
    "        x_k_1 = x_k\n",
    "        x_k = x_k + alpha_k * dx_k\n",
    "\n",
    "        # Store iteration info\n",
    "        info['xs'].append(x_k)\n",
    "        info['alphas'].append(alpha_k)\n",
    "\n",
    "        # Check stopping condition\n",
    "        stopCond = np.linalg.norm(x_k - x_k_1) / np.linalg.norm(x_k_1) < tol\n",
    "\n",
    "    # Assign output values\n",
    "    xMin = x_k\n",
    "    fMin = F['f'](x_k)\n",
    "    return xMin, fMin, nIter, info\n",
    "\n",
    "\n",
    "def backtracking(F, x_k, p_k, alpha0, lsOpts):\n",
    "    \"\"\"\n",
    "    Backtracking line search.\n",
    "\n",
    "    Parameters:\n",
    "    F (dict): Dictionary containing function and gradient.\n",
    "    x_k (numpy.ndarray): Current point.\n",
    "    p_k (numpy.ndarray): Search direction.\n",
    "    alpha0 (float): Initial step length.\n",
    "    lsOpts (dict): Line search options.\n",
    "\n",
    "    Returns:\n",
    "    alpha_k (float): Step length.\n",
    "    \"\"\"\n",
    "    rho = lsOpts['rho']\n",
    "    c1 = lsOpts['c1']\n",
    "    alpha_k = alpha0\n",
    "    while F['f'](x_k + alpha_k * p_k) > F['f'](x_k) + c1 * alpha_k * F['df'](x_k) @ p_k:\n",
    "        alpha_k *= rho\n",
    "    return alpha_k\n",
    "\n",
    "\n",
    "# Define the objective function and its derivatives\n",
    "def f(x):\n",
    "    return (x[0] - 2 * x[1])**2 + (x[0]**2 + 2)**2\n",
    "\n",
    "def df(x):\n",
    "    return np.array([10 * x[0] + 4 * x[0]**3 - 4 * x[1], -4 * (x[0] - 2 * x[1])])\n",
    "\n",
    "def d2f(x):\n",
    "    return np.array([[10 + 12 * x[0]**2, -4], [-4, 8]])\n",
    "\n",
    "# Define the equality constraint\n",
    "eqConstraint = {'A': np.array([[1, -1]]), 'b': 4}\n",
    "\n",
    "# Initialization\n",
    "x0 = np.array([-4, -8])\n",
    "alpha0 = 1\n",
    "tol = 1e-6\n",
    "maxIter = 20\n",
    "\n",
    "# Define the line search options\n",
    "lsOptsNewton = {'rho': 0.9, 'c1': 1e-4}\n",
    "\n",
    "# Define the line search function\n",
    "lsFun = lambda x_k, p_k, alpha0: backtracking({'f': f, 'df': df}, x_k, p_k, alpha0, lsOptsNewton)\n",
    "\n",
    "# Solve the optimization problem\n",
    "xMin, fMin, nIter, info = feasibleNewton({'f': f, 'df': df, 'd2f': d2f}, eqConstraint, lsFun, alpha0, x0, tol, maxIter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6e5863",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3230129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Objective function\n",
    "def objective(x):\n",
    "    a, b = 1, 1.5\n",
    "    return (x[0] - a) ** 2 + (x[1] - b) ** 2 / 2 - 1\n",
    "\n",
    "# Inequality constraints\n",
    "def constraint_C(x):\n",
    "    return x[0] ** 2 + x[1] ** 2 - 2\n",
    "\n",
    "def constraint_L(x):\n",
    "    return -x[0] + x[1] + 1\n",
    "\n",
    "# Equality constraint\n",
    "def eq_constraint(x):\n",
    "    return 3 / 2 * x[0] - x[1] - 2\n",
    "\n",
    "# Combine constraints\n",
    "constraints = ({'type': 'ineq', 'fun': constraint_C},\n",
    "               {'type': 'ineq', 'fun': constraint_L},\n",
    "               {'type': 'eq', 'fun': eq_constraint})\n",
    "\n",
    "# Initial guess\n",
    "x0 = np.array([1/2, 3/2*1/2-2])\n",
    "\n",
    "# Minimize the objective function subject to constraints using SLSQP method\n",
    "result = minimize(objective, x0, method='SLSQP', constraints=constraints, tol=1e-12, options={'maxiter': 200})\n",
    "\n",
    "# Print the result\n",
    "print(\"Optimal solution:\", result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc733fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e135082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9969046",
   "metadata": {},
   "source": [
    "# Tutorial 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856ae88b",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84003ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Objective function\n",
    "def objective(x, a, b):\n",
    "    return (x[0] - a) ** 2 + (x[1] - b) ** 2 / 2 - 1\n",
    "\n",
    "# Constraint function (circle)\n",
    "def constraint(x, xC, yC, rC2):\n",
    "    return (x[0] - xC) ** 2 + (x[1] - yC) ** 2 - rC2\n",
    "\n",
    "# Solve using SciPy's minimize function\n",
    "def solve(a, b, xC, yC, rC2):\n",
    "    # Initial guess\n",
    "    x0 = np.array([a, b])\n",
    "    \n",
    "    # Objective function with additional parameters (a, b)\n",
    "    obj_func = lambda x: objective(x, a, b)\n",
    "    \n",
    "    # Constraint function with additional parameters (xC, yC, rC2)\n",
    "    cons = ({'type': 'eq', 'fun': lambda x: constraint(x, xC, yC, rC2)})\n",
    "    \n",
    "    # Minimize the objective function subject to the constraint\n",
    "    result = minimize(obj_func, x0, constraints=cons)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Parameters\n",
    "a = 1\n",
    "b = 1.5\n",
    "xC = 0\n",
    "yC = 0\n",
    "rC2 = 2\n",
    "\n",
    "# Solve the problem\n",
    "result = solve(a, b, xC, yC, rC2)\n",
    "\n",
    "# Print the result\n",
    "print(\"Optimal solution:\", result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceedfdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "164adc55",
   "metadata": {},
   "source": [
    "### Augmented lagrangian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def augmentedLagrangian(F, eqConstraint, x0, mu, nu0, tol, maxIter):\n",
    "    nu = nu0\n",
    "    nIter = 0\n",
    "    x_k = x0\n",
    "    x_k_1 = x0\n",
    "    xs = [x0]\n",
    "    nus = [nu0]\n",
    "    fs = [F['f'](x0) + nu * eqConstraint['f'](x0) + mu / 2 * eqConstraint['f'](x0) ** 2]\n",
    "\n",
    "    # Line search parameters\n",
    "    alpha0 = 1\n",
    "    opts = {'c1': 1e-4, 'c2': 0.9, 'rho': 0.9}\n",
    "\n",
    "    # Stopping condition: norm(x_k - x_k_1) < tol\n",
    "    stopCond = False\n",
    "\n",
    "    while nIter < maxIter and not stopCond:\n",
    "        nIter += 1\n",
    "\n",
    "        # Save previous iterate\n",
    "        x_k_1 = x_k\n",
    "\n",
    "        # Augmented Lagrangian function\n",
    "        FALag = {}\n",
    "        FALag['f'] = lambda x: F['f'](x) + nu * eqConstraint['f'](x) + mu / 2 * eqConstraint['f'](x) ** 2\n",
    "        FALag['df'] = lambda x: F['df'](x) + nu * eqConstraint['df'](x) + mu * eqConstraint['f'](x) * eqConstraint['df'](x)\n",
    "        FALag['d2f'] = lambda x: F['d2f'](x) + nu * eqConstraint['d2f'](x) + mu * eqConstraint['df'](x) @ eqConstraint['df'](x).T + mu * eqConstraint['f'](x) * eqConstraint['d2f'](x)\n",
    "\n",
    "        # Solve using constrained optimization method\n",
    "        result = minimize(FALag['f'], x_k, jac=FALag['df'], hess=FALag['d2f'], method='Newton-CG', tol=tol, options=opts)\n",
    "        x_k = result.x\n",
    "        xs.append(x_k)\n",
    "\n",
    "        # Update nu\n",
    "        nu += mu * eqConstraint['f'](x_k)\n",
    "        nus.append(nu)\n",
    "\n",
    "        # Evaluate stopping condition\n",
    "        stopCond = np.linalg.norm(x_k - x_k_1) < tol\n",
    "\n",
    "        # Compute function value\n",
    "        f_k = F['f'](x_k)\n",
    "        fs.append(f_k)\n",
    "\n",
    "    xMin = x_k\n",
    "    fMin = F['f'](x_k)\n",
    "\n",
    "    info = {'xs': np.array(xs), 'nus': np.array(nus), 'fs': np.array(fs)}\n",
    "    return xMin, fMin, nIter, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38707c9",
   "metadata": {},
   "source": [
    "### Quadratic Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7430516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def quadraticPenalty(F, eqConstraint, x0, mu0, beta, tol, maxIter, alpha0, opts):\n",
    "    mu = mu0\n",
    "    tau = 10e3 * tol\n",
    "    nIter = 0\n",
    "    x_k = x0\n",
    "    x_k_1 = x0\n",
    "    stopCond = False\n",
    "\n",
    "    xs = [x0]\n",
    "    mus = [mu0]\n",
    "    FQPen = {}\n",
    "    FQPen['mu'] = mu0\n",
    "    FQPen['f'] = lambda x: F['f'](x) + FQPen['mu'] / 2 * eqConstraint['f'](x) ** 2\n",
    "    FQPen['df'] = lambda x: F['df'](x) + FQPen['mu'] * eqConstraint['f'](x) * eqConstraint['df'](x)\n",
    "    FQPen['d2f'] = lambda x: F['d2f'](x) + FQPen['mu'] * eqConstraint['df'](x) @ eqConstraint['df'](x).T + FQPen['mu'] * eqConstraint['f'](x) * eqConstraint['d2f'](x)\n",
    "\n",
    "    fs = [FQPen['f'](x0)]\n",
    "\n",
    "    while nIter < maxIter and not stopCond:\n",
    "        nIter += 1\n",
    "\n",
    "        x_k_1 = x_k\n",
    "\n",
    "        FQPen['mu'] = mu\n",
    "        FQPen['f'] = lambda x: F['f'](x) + FQPen['mu'] / 2 * eqConstraint['f'](x) ** 2\n",
    "        FQPen['df'] = lambda x: F['df'](x) + FQPen['mu'] * eqConstraint['f'](x) * eqConstraint['df'](x)\n",
    "        FQPen['d2f'] = lambda x: F['d2f'](x) + FQPen['mu'] * eqConstraint['df'](x) @ eqConstraint['df'](x).T + FQPen['mu'] * eqConstraint['f'](x) * eqConstraint['d2f'](x)\n",
    "\n",
    "        lsFun = lambda x_k, p_k, alpha0: backtracking(FQPen, x_k, p_k, alpha0, opts)\n",
    "        result = minimize(FQPen['f'], x_k, jac=FQPen['df'], hess=FQPen['d2f'], method='Newton-CG', tol=tau, options={'maxiter': 100}, args=(), callback=None)\n",
    "\n",
    "        x_k = result.x\n",
    "        xs.append(x_k)\n",
    "\n",
    "        f_k = F['f'](x_k)\n",
    "        fs.append(f_k)\n",
    "\n",
    "        mu *= beta\n",
    "        mus.append(mu)\n",
    "\n",
    "        stopCond = np.linalg.norm(x_k - x_k_1) < tol\n",
    "        tau /= beta\n",
    "\n",
    "    xMin = x_k\n",
    "    fMin = F['f'](x_k)\n",
    "\n",
    "    info = {'xs': np.array(xs), 'mus': np.array(mus), 'fs': np.array(fs)}\n",
    "    return xMin, fMin, nIter, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b97023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
